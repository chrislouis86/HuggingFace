{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqNLsoThqrOY"
      },
      "source": [
        "# Hands-On Exercises: Fine-Tuning SmolLM3\n",
        "\n",
        "Welcome to the practical section! Here you'll apply everything you've learned about chat templates and supervised fine-tuning using SmolLM3. These exercises progress from basic concepts to advanced techniques, giving you real-world experience with instruction tuning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9CJ12VJqrOZ"
      },
      "source": [
        "## Learning Objectives\n",
        "\n",
        "By completing these exercises, you will:\n",
        "- Master SmolLM3's chat template system\n",
        "- Fine-tune SmolLM3 on real datasets using both Python APIs and CLI tools\n",
        "- Work with the SmolTalk2 dataset that was used to train the original model\n",
        "- Compare base model vs fine-tuned model performance\n",
        "- Deploy your models to Hugging Face Hub\n",
        "- Understand production workflows for scaling fine-tuning\n",
        "\n",
        "---\n",
        "\n",
        "## Exercise 1: Exploring SmolLM3's Chat Templates\n",
        "\n",
        "**Objective**: Understand how SmolLM3 handles different conversation formats and reasoning modes.\n",
        "\n",
        "SmolLM3 is a hybrid reasoning model which can follow instructions or generated tokens that 'reason' on a complex problem. When post-trained effectively, the model will reason on hard problems and generate direct responses on easy problems.\n",
        "\n",
        "### Environment Setup\n",
        "\n",
        "Let's start by setting up our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bkgb26GqqrOZ",
        "outputId": "72d25c41-0c07-44a6-b8f3-0dfad24b8221"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m564.7/564.7 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m859.7/859.7 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install required packages (run in Colab or your environment)\n",
        "!pip install -qqq \"transformers>=4.55.0\" \"trl>=0.22.1\" \"datasets\" \"torch\"\n",
        "!pip install -qqq \"accelerate\" \"peft\" \"trackio\" \"huggingface_hub\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7bX1gt7qrOa",
        "outputId": "3a84769c-c519-45ed-b2f1-ac259c480206"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CUDA GPU: Tesla T4\n",
            "GPU memory: 15.8GB\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "    print(f\"Using CUDA GPU: {torch.cuda.get_device_name()}\")\n",
        "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
        "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "    print(\"Using Apple MPS\")\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    print(\"Using CPU - you will need to use a GPU to train models\")\n",
        "\n",
        "# Authenticate with Hugging Face (optional, for private models)\n",
        "from huggingface_hub import login\n",
        "# login()  # Uncomment if you need to access private models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkdhXXI8qrOa"
      },
      "source": [
        "### Load SmolLM3 Models\n",
        "\n",
        "Now let's load the base and instruct models for comparison.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "id": "DW2B1bL6qrOa",
        "outputId": "544a80bc-535f-4ffd-e5fb-91064430d696"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "There was a specific connection error when trying to load HuggingFaceTB/SmolLM3-3B-Base:\n401 Client Error: Unauthorized for url: https://huggingface.co/HuggingFaceTB/SmolLM3-3B-Base/resolve/main/config.json (Request ID: Root=1-68dac01d-5e56b4917545449d787a24b3;578b81b0-324b-48ee-96ae-a57dc294c657)\n\nInvalid credentials in Authorization header",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/HuggingFaceTB/SmolLM3-3B-Base/resolve/main/config.json",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0;31m# This is slightly better for only 1 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m             hf_hub_download(\n\u001b[0m\u001b[1;32m    479\u001b[0m                 \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m   1011\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0;31m# Otherwise, raise appropriate error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m         \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1657\u001b[0m         \u001b[0;31m# Unauthorized => likely a token issue => let's raise the actual error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mhead_call_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1659\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1545\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1546\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1547\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers, endpoint)\u001b[0m\n\u001b[1;32m   1462\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1463\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1464\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    287\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_backoff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry_on_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry_on_status_codes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m429\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;31m# as well (request id and/or server error message)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHfHubHTTPError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHfHubHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/HuggingFaceTB/SmolLM3-3B-Base/resolve/main/config.json (Request ID: Root=1-68dac01d-5e56b4917545449d787a24b3;578b81b0-324b-48ee-96ae-a57dc294c657)\n\nInvalid credentials in Authorization header",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3827115240.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minstruct_model_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"HuggingFaceTB/SmolVLM2-2.2B-Instruct\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load tokenizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mbase_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0minstruct_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstruct_model_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1076\u001b[0m                     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1078\u001b[0;31m                     config = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m   1079\u001b[0m                         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1286\u001b[0m         \u001b[0mcode_revision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"code_revision\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1288\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1289\u001b[0m         \u001b[0mhas_remote_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"auto_map\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"AutoConfig\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"auto_map\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m         \u001b[0mhas_local_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0moriginal_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;31m# Get config dict associated with the base config file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    719\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;31m# Load from local folder or from cache or download from model Hub and cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 721\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    722\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m                     \u001b[0mconfiguration_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \"\"\"\n\u001b[0;32m--> 321\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_raise_exceptions_for_connection_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"There was a specific connection error when trying to load {path_or_repo_id}:\\n{e}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m         \u001b[0;31m# Any other Exception type should now be re-raised, in order to provide helpful error messages and break the execution flow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;31m# (EntryNotFoundError will be treated outside this block and correctly re-raised if needed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: There was a specific connection error when trying to load HuggingFaceTB/SmolLM3-3B-Base:\n401 Client Error: Unauthorized for url: https://huggingface.co/HuggingFaceTB/SmolLM3-3B-Base/resolve/main/config.json (Request ID: Root=1-68dac01d-5e56b4917545449d787a24b3;578b81b0-324b-48ee-96ae-a57dc294c657)\n\nInvalid credentials in Authorization header"
          ]
        }
      ],
      "source": [
        "# Load both base and instruct models for comparison\n",
        "base_model_name = \"HuggingFaceTB/SmolLM3-3B-Base\"\n",
        "instruct_model_name = \"HuggingFaceTB/SmolVLM2-2.2B-Instruct\"\n",
        "# Load tokenizers\n",
        "base_tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "instruct_tokenizer = AutoTokenizer.from_pretrained(instruct_model_name)\n",
        "\n",
        "# Load models (use smaller precision for memory efficiency)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "instruct_model = AutoModelForCausalLM.from_pretrained(\n",
        "    instruct_model_name,\n",
        "    dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "print(\"Models loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiYWJcZzqrOa"
      },
      "source": [
        "### Explore Chat Template Formatting\n",
        "\n",
        "Now let's explore the chat template formatting. We will create different types of conversations to test.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "eS6Ms948qrOa",
        "outputId": "2cdea30d-8902-41d8-933e-7c75f8b52900"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- SIMPLE_QA ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'instruct_tokenizer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3544703332.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# Format without generation prompt (for completed conversations)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     formatted_complete = instruct_tokenizer.apply_chat_template(\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_generation_prompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'instruct_tokenizer' is not defined"
          ]
        }
      ],
      "source": [
        "# Create different types of conversations to test\n",
        "conversations = {\n",
        "    \"simple_qa\": [\n",
        "        {\"role\": \"system\", \"content\": \"/no_think\"},\n",
        "        {\"role\": \"user\", \"content\": \"What is machine learning?\"},\n",
        "    ],\n",
        "    \"with_system\": [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful AI assistant specialized in explaining technical concepts clearly. /no_think\",\n",
        "        },\n",
        "        {\"role\": \"user\", \"content\": \"What is machine learning?\"},\n",
        "    ],\n",
        "    \"multi_turn\": [\n",
        "        {\"role\": \"system\", \"content\": \"You are a math tutor. /no_think\"},\n",
        "        {\"role\": \"user\", \"content\": \"What is calculus?\"},\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": \"Calculus is a branch of mathematics that deals with rates of change and accumulation of quantities.\",\n",
        "        },\n",
        "        {\"role\": \"user\", \"content\": \"Can you give me a simple example?\"},\n",
        "    ],\n",
        "    \"reasoning_task\": [\n",
        "        {\"role\": \"system\", \"content\": \"/think\"},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Solve step by step: If a train travels 120 miles in 2 hours, what is its average speed?\",\n",
        "        },\n",
        "    ],\n",
        "}\n",
        "\n",
        "for conv_type, messages in conversations.items():\n",
        "    print(f\"--- {conv_type.upper()} ---\")\n",
        "\n",
        "    # Format without generation prompt (for completed conversations)\n",
        "    formatted_complete = instruct_tokenizer.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=False\n",
        "    )\n",
        "\n",
        "    # Format with generation prompt (for inference)\n",
        "    formatted_prompt = instruct_tokenizer.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    print(\"Complete conversation format:\")\n",
        "    print(formatted_complete)\n",
        "    print(\"\\nWith generation prompt:\")\n",
        "    print(formatted_prompt)\n",
        "    print(\"\\n\" + \"=\" * 50 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U46yJA99qrOa"
      },
      "source": [
        "**Step 4: Compare Base vs Instruct Model Responses**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcSaswqXqrOa"
      },
      "outputs": [],
      "source": [
        "# Test the same prompt on both models\n",
        "test_prompt = \"Explain quantum computing in simple terms.\"\n",
        "\n",
        "# Prepare the prompt for base model (no chat template)\n",
        "base_inputs = base_tokenizer(test_prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Prepare the prompt for instruct model (with chat template)\n",
        "instruct_messages = [\n",
        "    {\"role\": \"system\", \"content\": \"/no_think\"},\n",
        "    {\"role\": \"user\", \"content\": test_prompt}\n",
        "]\n",
        "instruct_formatted = instruct_tokenizer.apply_chat_template(\n",
        "    instruct_messages, tokenize=False, add_generation_prompt=True\n",
        ")\n",
        "instruct_inputs = instruct_tokenizer(instruct_formatted, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Generate responses\n",
        "print(\"=== Model comparison ===\\n\")\n",
        "\n",
        "print(\"ðŸ¤– BASE MODEL RESPONSE:\")\n",
        "with torch.no_grad():\n",
        "    base_outputs = base_model.generate(\n",
        "        **base_inputs,\n",
        "        max_new_tokens=150,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        pad_token_id=base_tokenizer.eos_token_id,\n",
        "    )\n",
        "    base_response = base_tokenizer.decode(base_outputs[0], skip_special_tokens=True)\n",
        "    print(base_response[len(test_prompt) :])  # Show only the generated part\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Instruct model response:\")\n",
        "with torch.no_grad():\n",
        "    instruct_outputs = instruct_model.generate(\n",
        "        **instruct_inputs,\n",
        "        max_new_tokens=150,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        pad_token_id=instruct_tokenizer.eos_token_id,\n",
        "    )\n",
        "    instruct_response = instruct_tokenizer.decode(\n",
        "        instruct_outputs[0], skip_special_tokens=True\n",
        "    )\n",
        "    # Extract only the assistant's response\n",
        "    assistant_start = instruct_response.find(\"<|im_start|>assistant\\n\") + len(\n",
        "        \"<|im_start|>assistant\\n\"\n",
        "    )\n",
        "    assistant_response = instruct_response[assistant_start:]\n",
        "    print(assistant_response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2q9bQG45qrOb"
      },
      "source": [
        "**Step 5: Test Dual-Mode Reasoning**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNZwGdcmqrOb"
      },
      "outputs": [],
      "source": [
        "# Test SmolLM3's reasoning capabilities\n",
        "reasoning_prompts = [\n",
        "    \"What is 15 Ã— 24? Show your work.\",\n",
        "    \"A recipe calls for 2 cups of flour for 12 cookies. How much flour is needed for 30 cookies?\",\n",
        "    \"If I have $50 and spend $18.75 on lunch and $12.30 on a book, how much money do I have left?\",\n",
        "]\n",
        "\n",
        "thinking_prompts = [\n",
        "    \"/no_think\",\n",
        "    \"/think\"\n",
        "]\n",
        "\n",
        "print(\"=== TESTING REASONING CAPABILITIES ===\\n\")\n",
        "\n",
        "for thinking_prompt in thinking_prompts:\n",
        "    print(f\"Thinking prompt: {thinking_prompt}\")\n",
        "    for i, prompt in enumerate(reasoning_prompts, 1):\n",
        "        print(f\"Problem {i}: {prompt}\")\n",
        "\n",
        "        messages = [\n",
        "            {\"role\":\"system\", \"content\": thinking_prompt},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "        formatted_prompt = instruct_tokenizer.apply_chat_template(\n",
        "            messages, tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "        inputs = instruct_tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = instruct_model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=200,\n",
        "                temperature=0.3,  # Lower temperature for more consistent reasoning\n",
        "                do_sample=True,\n",
        "                pad_token_id=instruct_tokenizer.eos_token_id,\n",
        "            )\n",
        "            response = instruct_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            assistant_start = response.find(\"<|im_start|>assistant\\n\") + len(\n",
        "                \"<|im_start|>assistant\\n\"\n",
        "            )\n",
        "            assistant_response = response[assistant_start:].split(\"<|im_end|>\")[0]\n",
        "            print(f\"Answer: {assistant_response}\")\n",
        "\n",
        "        print(\"\\n\" + \"-\" * 50 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZBhO38uqrOb"
      },
      "source": [
        "### Validation\n",
        "\n",
        "Run the code above and verify that you can see:\n",
        "1. Different chat template formats for various conversation types\n",
        "2. Clear differences between base model and instruct model responses\n",
        "3. SmolLM3's reasoning capabilities in action\n",
        "\n",
        "### Extension challenges\n",
        "\n",
        "1. **Multilingual Testing**: Test SmolLM3's multilingual capabilities by asking questions in French, Spanish, or German\n",
        "2. **Long Context**: Create a very long conversation and test the extended context capabilities\n",
        "3. **Custom System Prompts**: Experiment with different system messages to change the model's behavior\n",
        "\n",
        "---\n",
        "\n",
        "## Exercise 2: Dataset Processing for SFT\n",
        "\n",
        "**Objective**: Learn to process and prepare datasets for supervised fine-tuning using SmolTalk2 and other datasets.\n",
        "\n",
        "**Prerequisites**: Completed Exercise 1, understanding of Python data processing.\n",
        "\n",
        "### Implementation\n",
        "\n",
        "**Step 1: Explore the SmolTalk2 Dataset**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvFF3BT5qrOb"
      },
      "outputs": [],
      "source": [
        "# Load and explore the SmolTalk2 dataset\n",
        "print(\"=== EXPLORING SMOLTALK2 DATASET ===\\n\")\n",
        "\n",
        "# Load the SFT subset\n",
        "dataset_dict = load_dataset(\"HuggingFaceTB/smoltalk2\", \"SFT\")\n",
        "print(f\"Total splits: {len(dataset_dict)}\")\n",
        "print(f\"Available splits: {list(dataset_dict.keys())}\")\n",
        "print(f\"Number of total rows: {sum([dataset_dict[d].num_rows for d in dataset_dict])}\")\n",
        "print(f\"Dataset structure: {dataset_dict}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TvDiw5vqrOb"
      },
      "outputs": [],
      "source": [
        "# Function to process different dataset formats\n",
        "def process_qa_dataset(examples, question_col, answer_col):\n",
        "    \"\"\"Process Q&A datasets into chat format\"\"\"\n",
        "    processed = []\n",
        "\n",
        "    for question, answer in zip(examples[question_col], examples[answer_col]):\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": question},\n",
        "            {\"role\": \"assistant\", \"content\": answer},\n",
        "        ]\n",
        "        processed.append(messages)\n",
        "\n",
        "    return {\"messages\": processed}\n",
        "\n",
        "\n",
        "def process_instruction_dataset(examples):\n",
        "    \"\"\"Process instruction-following datasets\"\"\"\n",
        "    processed = []\n",
        "\n",
        "    for instruction, response in zip(examples[\"instruction\"], examples[\"response\"]):\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": instruction},\n",
        "            {\"role\": \"assistant\", \"content\": response},\n",
        "        ]\n",
        "        processed.append(messages)\n",
        "\n",
        "    return {\"messages\": processed}\n",
        "\n",
        "\n",
        "# Example: Process GSM8K math dataset\n",
        "print(\"=== PROCESSING GSM8K DATASET ===\\n\")\n",
        "\n",
        "gsm8k = load_dataset(\n",
        "    \"openai/gsm8k\", \"main\", split=\"train[:100]\"\n",
        ")  # Small subset for demo\n",
        "print(f\"Original GSM8K example: {gsm8k[0]}\")\n",
        "\n",
        "\n",
        "# Convert to chat format\n",
        "def process_gsm8k(examples):\n",
        "    processed = []\n",
        "    for question, answer in zip(examples[\"question\"], examples[\"answer\"]):\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a math tutor. Solve problems step by step.\",\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": question},\n",
        "            {\"role\": \"assistant\", \"content\": answer},\n",
        "        ]\n",
        "        processed.append(messages)\n",
        "    return {\"messages\": processed}\n",
        "\n",
        "\n",
        "gsm8k_processed = gsm8k.map(\n",
        "    process_gsm8k, batched=True, remove_columns=gsm8k.column_names\n",
        ")\n",
        "print(f\"Processed example: {gsm8k_processed[0]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zn83tgUXqrOb"
      },
      "outputs": [],
      "source": [
        "# Function to apply chat templates to processed datasets\n",
        "def apply_chat_template_to_dataset(dataset, tokenizer):\n",
        "    \"\"\"Apply chat template to dataset for training\"\"\"\n",
        "\n",
        "    def format_messages(examples):\n",
        "        formatted_texts = []\n",
        "\n",
        "        for messages in examples[\"messages\"]:\n",
        "            # Apply chat template\n",
        "            formatted_text = tokenizer.apply_chat_template(\n",
        "                messages,\n",
        "                tokenize=False,\n",
        "                add_generation_prompt=False,  # We want the complete conversation\n",
        "            )\n",
        "            formatted_texts.append(formatted_text)\n",
        "\n",
        "        return {\"text\": formatted_texts}\n",
        "\n",
        "    return dataset.map(format_messages, batched=True)\n",
        "\n",
        "\n",
        "# Apply to our processed GSM8K dataset\n",
        "gsm8k_formatted = apply_chat_template_to_dataset(gsm8k_processed, instruct_tokenizer)\n",
        "print(\"=== FORMATTED TRAINING DATA ===\")\n",
        "print(gsm8k_formatted[0][\"text\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbOv4ynfqrOb"
      },
      "source": [
        "---\n",
        "\n",
        "## Exercise 3: Fine-Tuning SmolLM3 with SFTTrainer\n",
        "\n",
        "**Objective**: Perform supervised fine-tuning on SmolLM3 using TRL's SFTTrainer with real datasets.\n",
        "\n",
        "**Prerequisites**: Completed Exercise 2, GPU with at least 8GB VRAM (or Google Colab Pro).\n",
        "\n",
        "### Implementation\n",
        "\n",
        "**Step 1: Setup and Model Loading**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPjj4FGbqrOb"
      },
      "outputs": [],
      "source": [
        "# Import required libraries for fine-tuning\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "\n",
        "# Load SmolLM3 base model for fine-tuning\n",
        "model_name = \"HuggingFaceTB/SmolLM3-3B\"\n",
        "new_model_name = \"SmolLM3-Custom-SFT\"\n",
        "\n",
        "print(f\"Loading {model_name}...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    dtype=torch.float16,  # Use float16 for memory efficiency\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Set padding token\n",
        "tokenizer.padding_side = \"right\"  # Padding on the right for generation\n",
        "\n",
        "print(f\"Model loaded! Parameters: {model.num_parameters():,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XXvO_bNqrOb"
      },
      "source": [
        "**Step 2: Dataset Preparation**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zj-rA2qkqrOc"
      },
      "outputs": [],
      "source": [
        "# Load and prepare training dataset\n",
        "print(\"=== PREPARING DATASET ===\\n\")\n",
        "\n",
        "# Option 1: Use SmolTalk2 (recommended for beginners)\n",
        "dataset = load_dataset(\"HuggingFaceTB/smoltalk2\", \"SFT\")\n",
        "training_split = \"smoltalk_everyday_convs_reasoning_Qwen3_32B_think\"\n",
        "train_dataset = dataset[training_split].select(range(1000))  # Use subset for faster training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggVPP7Hw4xcF"
      },
      "outputs": [],
      "source": [
        "# Configure training parameters\n",
        "training_config = SFTConfig(\n",
        "    # Model and data\n",
        "    output_dir=f\"./{new_model_name}\",\n",
        "    dataset_text_field=\"text\",\n",
        "    max_length=2048,\n",
        "\n",
        "    # Training hyperparameters\n",
        "    per_device_train_batch_size=2,  # Adjust based on your GPU memory\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=5e-5,\n",
        "    num_train_epochs=1,  # Start with 1 epoch\n",
        "    max_steps=500,  # Limit steps for demo\n",
        "\n",
        "    # Optimization\n",
        "    warmup_steps=50,\n",
        "    weight_decay=0.01,\n",
        "    optim=\"adamw_torch\",\n",
        "\n",
        "    # Logging and saving\n",
        "    logging_steps=10,\n",
        "    save_steps=100,\n",
        "    eval_steps=100,\n",
        "    save_total_limit=2,\n",
        "\n",
        "    # Memory optimization\n",
        "    dataloader_num_workers=0,\n",
        "    group_by_length=True,  # Group similar length sequences\n",
        "\n",
        "    # Hugging Face Hub integration\n",
        "    push_to_hub=False,  # Set to True to upload to Hub\n",
        "    hub_model_id=f\"your-username/{new_model_name}\",\n",
        "\n",
        "    # Experiment tracking\n",
        "    report_to=[\"trackio\"],  # Use trackio for experiment tracking\n",
        "    run_name=f\"{new_model_name}-training\",\n",
        ")\n",
        "\n",
        "print(\"Training configuration set!\")\n",
        "print(f\"Effective batch size: {training_config.per_device_train_batch_size * training_config.gradient_accumulation_steps}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzi4QTfb5Nv1"
      },
      "outputs": [],
      "source": [
        "# Initialize the SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_config,\n",
        "    train_dataset=train_dataset,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JReniw7Y5Z6m"
      },
      "outputs": [],
      "source": [
        "# Start training!\n",
        "print(\"\\n=== STARTING TRAINING ===\")\n",
        "trainer.train()\n",
        "\n",
        "# Save the model\n",
        "trainer.save_model()\n",
        "print(f\"Model saved to {training_config.output_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvFPZUa8qrOc"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xj04EhJeMGO"
      },
      "source": [
        "# LoRA SFT with TRL + SmolLM3\n",
        "\n",
        "This short notebook shows how to fine-tune a small model with LoRA adapters using TRL's SFTTrainer. It uses a tiny model (SmolLM2-135M) and a small public chat dataset for a quick demonstration.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-33n4ZSeMGO"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxrVolLfeMGP"
      },
      "outputs": [],
      "source": [
        "# LoRA config\n",
        "peft_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# SFT config (short run)\n",
        "sft_config = SFTConfig(\n",
        "    output_dir=\"./smollm2-lora-demo\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=2,\n",
        "    packing=True,\n",
        "    learning_rate=2e-4,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"no\",\n",
        "    report_to=\"none\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3l5UOVt9eMGP"
      },
      "outputs": [],
      "source": [
        "# Trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=sft_config,\n",
        "    train_dataset=train_dataset,\n",
        "    peft_config=peft_config,\n",
        "    dataset_kwargs={\n",
        "        \"add_special_tokens\": False,\n",
        "        \"append_concat_token\": False,\n",
        "    },\n",
        ")\n",
        "\n",
        "# Short demo train\n",
        "trainer.train()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}